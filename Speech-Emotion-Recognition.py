# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IRonjAaVIexF_kjLf5zPp4kz2buWYnqz
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip '/content/drive/MyDrive/Crema/Crema.zip' -d 'Crema'

import os
import re
from tqdm.notebook import tqdm as tq
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from IPython.display import Audio
from IPython.core.display import display
from keras.models import Sequential
from keras.layers import Dense, Conv1D, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D
from keras.utils import np_utils
from keras.callbacks import ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder,LabelEncoder, StandardScaler
from tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint
from tensorflow.keras import layers
from tensorflow.keras import regularizers, initializers
from tensorflow.keras import optimizers
from tensorflow.keras.layers import MaxPool1D, MaxPooling1D, AveragePooling1D, Activation
import itertools

Crema = "/content/drive/MyDrive/Crema/"
# Crema = "Crema"
crema_directory_list = os.listdir(Crema)

file_emotion = []
file_path = []

for file in crema_directory_list:
    # storing file paths
    file_path.append(Crema + file)
    # storing file emotions
    part=file.split('_')
    if part[2] == 'SAD':
        file_emotion.append('sad')
    elif part[2] == 'ANG':
        file_emotion.append('angry')
    elif part[2] == 'DIS':
        file_emotion.append('disgust')
    elif part[2] == 'FEA':
        file_emotion.append('fear')
    elif part[2] == 'HAP':
        file_emotion.append('happy')
    elif part[2] == 'NEU':
        file_emotion.append('neutral')
    else:
        file_emotion.append('Unknown')
        
# dataframe for emotion of files
emotion_df = pd.DataFrame(file_emotion, columns=['Emotion'])

# dataframe for path of files.
path_df = pd.DataFrame(file_path, columns=['Path'])
Crema_df = pd.concat([emotion_df, path_df], axis=1)
Crema_df.head()

def create_labeled_dataset():
  label_list = []
  for wav in os.listdir('Crema'):
    filename_list = wav.partition(".wav")[0].split("_")
    label = filename_list[2]
    if label == 'SAD':
      label_list.append(("sad", 'Crema' + "/" + wav))
    if label == 'ANG':
      label_list.append(("angry", 'Crema' + "/" + wav))
    if label == 'DIS':
      label_list.append(("disgust", 'Crema' + "/" + wav))
    if label == 'FEA':
      label_list.append(("fear", 'Crema' + "/" + wav))
    if label == 'HAP':
      label_list.append(("happy", 'Crema' + "/" + wav))
    if label == 'NEU':
      label_list.append(("neutral", 'Crema' + "/" + wav))
  Crema_df = pd.DataFrame().from_dict(label_list)
  Crema_df.rename(columns={1 : "Path", 0 : "Emotion"}, inplace=True)
  return Crema_df

Crema_df = create_labeled_dataset()
Crema_df.head()

Crema_df['Emotion'].value_counts()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.style.use("ggplot")
def create_waveplot(data, Fs, emotion):
  plt.figure(figsize=(10, 3))
  plt.title(f'Waveplot for audio with {emotion} emotion', size=15)
  librosa.display.waveplot(data, sr=Fs)
  plt.show()
  print()

def load_and_plot_audio(df, emotion):
  path = np.array(df.Path[df.Emotion == emotion])[1]
  data, sampling_rate = librosa.load(path)
  create_waveplot(data, sampling_rate, emotion)
  display(Audio(path))

load_and_plot_audio(Crema_df, 'sad')
load_and_plot_audio(Crema_df, 'angry')
load_and_plot_audio(Crema_df, 'disgust')
load_and_plot_audio(Crema_df, 'fear')
load_and_plot_audio(Crema_df, 'happy')
load_and_plot_audio(Crema_df, 'neutral')

def split_data_into_train_val_test(dataset, labels, test_size, val_size, ismel = False):
  '''
  makes the split of the dataset into train, val, and test sets.
  test_size: is percentage/100
  val_size: is percentage/100 of the train set after splitting the dataset into train and test sets
  returns a standardized version of X_train, X_val and X_test with their respective class labels (Y_train, Y_val, Y_test)
  '''
  X_train, X_test, y_train, y_test = train_test_split(dataset,
                                                      labels,
                                                      random_state=12,
                                                      test_size=test_size,
                                                      shuffle=True)
  
  X_train, X_val, y_train, y_val = train_test_split(X_train,
                                                    y_train,
                                                    random_state=12,
                                                    test_size=val_size,
                                                    shuffle=True)
  if ismel == False:
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)[..., np.newaxis]
    X_test = scaler.transform(X_test)[..., np.newaxis]
    X_val = scaler.transform(X_val)[..., np.newaxis]

  
  return X_train, X_val, X_test, y_train, y_val, y_test

"""# Making new features For Conv1D

## Helper Functions in feature extraction
"""

def zcr(data, frame_length=2048, hop_length=512):
    zcr = librosa.feature.zero_crossing_rate(y=data, frame_length=frame_length, hop_length=hop_length)
    return np.squeeze(zcr)

def energy(data, frame_length=2048, hop_length=512):
    en = np.array([np.sum(np.power(np.abs(data[hop:hop+frame_length]), 2)) for hop in range(0, data.shape[0], hop_length)])
    return en / frame_length
def rmse(data, frame_length=2048, hop_length=512):
    rmse = librosa.feature.rms(y=data, frame_length=frame_length, hop_length=hop_length)
    return np.squeeze(rmse)

def noise(data):
    noise_amp = 0.035*np.random.uniform()*np.amax(data)
    data = data + noise_amp*np.random.normal(size=data.shape[0])
    return data


def shift(data):
    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)
    return np.roll(data, shift_range)

def pitch(data, sampling_rate, pitch_factor=0.7):
    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)     
def stretch(data, rate=1):
    return librosa.effects.time_stretch(data, rate)

def extract_features(data, sr, frame_length=2048, hop_length=512):
  result = np.array([])
  result = np.hstack((result,zcr(data, frame_length, hop_length),rmse(data)))
  return result
  
def get_features(path, duration=2.5, offset = 0.6,augment=False):
  '''
    The function extracts the required features from the original audio files, Then adds noise, pitch  to all the dataset and extracts
    features from the noisy audio
  '''
  data, sample_rate = librosa.load(path, duration = duration, offset = offset)
  
  res1 = extract_features(data, np.array(sample_rate))
  
  result = np.array(res1)
  if augment:
  #data_augmentation
    noise_data = noise(data)

    res2=extract_features(noise_data, np.array(sample_rate))
  
    result = np.vstack((result, res2))
  
    stretched = stretch(data)

    pitched = pitch(stretched , sample_rate)
  
    res3 = extract_features(pitched,np.array(sample_rate))
  
    result = np.vstack((result,res3))
  
  return result

def get_data_features(df,augment,):
  data_elements, data_labels = [], []
  
  print("Feature processing...")
  for path, emotion, ind, i in zip(df.Path, df.Emotion, range(df.Path.shape[0]), tq(range(df.Path.shape[0]))):
    
    if augment==False:
      features = get_features(path)
      data_elements.append(features)
      data_labels.append(emotion)
    else:
      feature = get_features(path,augment=augment)
      for ele in feature:
        data_elements.append(ele)
        data_labels.append(emotion)
  print("Done.")
  return data_elements, data_labels

def save_features_to_csv_then_return(data_elements, data_labels, path):
  features_path = path
  new_Crema_df = pd.DataFrame(data_elements)
  new_Crema_df["labels"] = data_labels
  new_Crema_df = new_Crema_df.fillna(new_Crema_df.mean())
  new_Crema_df.to_csv(features_path, index=False)
  return new_Crema_df

def retrive_features_from_csv(path):
  new_Crema_df = pd.read_csv(path)
  return new_Crema_df

def prepare_data_and_labels(new_Crema_df):
  new_Crema_df = new_Crema_df.fillna(0)
  data_elements = new_Crema_df.drop(labels="labels", axis=1)
  data_labels = new_Crema_df["labels"]
  labels_encoder = LabelEncoder()
  labels_encoded = labels_encoder.fit_transform(data_labels)
  data_labels = np_utils.to_categorical(labels_encoded)
  print(labels_encoder.classes_)
  print(data_labels)
  return data_elements, data_labels

"""## Performing feature extraction to get the features for the Conv1D model

### Splitting before Augmentation
"""

X_train, X_test, y_train, y_test = train_test_split(Crema_df.Path, Crema_df.Emotion, shuffle = True, random_state = 12, test_size = 0.3)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, shuffle = True, random_state = 12, test_size =  0.05)
c_df = pd.DataFrame(X_train)
c_df["Emotion"] = y_train
X_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape

data_elements, data_labels = get_data_features(c_df,True)
new_Crema_df = save_features_to_csv_then_return(data_elements, data_labels, "/content/drive/MyDrive/Assign_Features/Train.csv")

X_train, y_train = prepare_data_and_labels(new_Crema_df)

t_df = pd.DataFrame(X_test)
t_df["Emotion"] = y_test
v_df = pd.DataFrame(X_val)
v_df["Emotion"] = y_val

X_val, y_val = get_data_features(v_df,augment=False)
new_Crema_df = save_features_to_csv_then_return(X_val, y_val, "/content/drive/MyDrive/Assign_Features/val.csv")
X_val, y_val = prepare_data_and_labels(new_Crema_df)

X_test, y_test = get_data_features(t_df,augment=False)
new_Crema_df = save_features_to_csv_then_return(X_test, y_test, "/content/drive/MyDrive/Assign_Features/Test.csv")
X_test, y_test = prepare_data_and_labels(new_Crema_df)

"""#### Retrive the features"""

#augmentation on Val & Train
new_Crema_df = retrive_features_from_csv('/content/drive/MyDrive/Assign_Features/Train3.csv')
data_elements, data_labels = prepare_data_and_labels(new_Crema_df)
X_train, X_val, y_train, y_val = train_test_split(data_elements, data_labels, shuffle = True, random_state = 12, test_size =  0.05)

new_Crema_df = retrive_features_from_csv('/content/drive/MyDrive/Assign_Features/Test.csv')
X_test, y_test = prepare_data_and_labels(new_Crema_df)

#train only
new_Crema_df = retrive_features_from_csv('/content/drive/MyDrive/Assign_Features/Train.csv')
X_train, y_train = prepare_data_and_labels(new_Crema_df)
new_Crema_df = retrive_features_from_csv('/content/drive/MyDrive/Assign_Features/val.csv')
X_val, y_val = prepare_data_and_labels(new_Crema_df)
new_Crema_df = retrive_features_from_csv('/content/drive/MyDrive/Assign_Features/Test.csv')
X_test, y_test = prepare_data_and_labels(new_Crema_df)

from sklearn import utils
X_train, y_train = utils.shuffle(X_train, y_train, random_state=14)
X_val, y_val = utils.shuffle(X_val, y_val, random_state=14)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)[..., np.newaxis]
X_test = scaler.transform(X_test)[..., np.newaxis]
X_val = scaler.transform(X_val)[..., np.newaxis]

X_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape

"""### Splitting after Augmentation"""

data_elements, data_labels = get_data_features(Crema_df,True)
new_Crema_df = save_features_to_csv_then_return(data_elements, data_labels, "/content/drive/MyDrive/Assign_Features/featuresConv1D.csv")
data_elements, data_labels = prepare_data_and_labels(new_Crema_df)
X_train, X_val, X_test, y_train, y_val, y_test = split_data_into_train_val_test(data_elements, data_labels, test_size=0.3, val_size=0.05, ismel = False)

X_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape

new_Crema_df = retrive_features_from_csv('/content/drive/MyDrive/Assign_Features/featuresConv1D22.csv')
data_elements, data_labels = prepare_data_and_labels(new_Crema_df)
X_train, X_val, X_test, y_train, y_val, y_test = split_data_into_train_val_test(data_elements, data_labels, test_size=0.3, val_size=0.05, ismel = False)

"""#For MelSpectroGram"""

def to_categroical_labels(data_labels):
  labels_encoder = LabelEncoder()
  labels_encoded = labels_encoder.fit_transform(data_labels)
  data_labels = np_utils.to_categorical(labels_encoded)
  print(labels_encoder.classes_)
  print(data_labels)
  return data_labels

"""##Function to create mel spectrograms"""

import cv2
def create_mel_spectrogram(data, sample_rate):
  mel_spectrogram = librosa.feature.melspectrogram(data, sr = sample_rate, n_fft=2048, n_mels=128)
  resized = cv2.resize(mel_spectrogram, (128, 108), interpolation = cv2.INTER_AREA)
  mel_spectrogram_db = librosa.power_to_db(resized)
  melnormalized = librosa.util.normalize(mel_spectrogram_db)
  return melnormalized



def show_random_melSpectrogram_emotion(emotion):
  path = np.array(Crema_df.Path[Crema_df.Emotion == emotion])[1]
  data, sample_rate = librosa.load(path, duration = 2.5, offset = 0.2)
  mel = create_mel_spectrogram(data, sample_rate)
  librosa.display.specshow(mel)
  plt.title(emotion)
  plt.show()


show_random_melSpectrogram_emotion('sad')
show_random_melSpectrogram_emotion('happy')
show_random_melSpectrogram_emotion('fear')
show_random_melSpectrogram_emotion('angry')
show_random_melSpectrogram_emotion('disgust')
show_random_melSpectrogram_emotion('neutral')

def get_all_mel_spectrograms(df, duration=2.5, offset = 0.2, augment = True):
  data_elements, data_labels = [], []
  for path, emotion, ind in zip(df.Path, df.Emotion, tq(range(df.Path.shape[0]))):
    data, sample_rate = librosa.load(path, duration = duration, offset = offset)
    if augment:
      noise_data = noise(data)
      stretched = stretch(data)
      pitched = pitch(stretched , sample_rate)
      for d in [data, noise_data, pitched]:
        spectrogram = create_mel_spectrogram(d, sample_rate)
        data_elements.append(spectrogram)
        data_labels.append(emotion)
    else:
      spectrogram = create_mel_spectrogram(data, sample_rate)
      data_elements.append(spectrogram)
      data_labels.append(emotion)      
  data_elements = np.asarray(data_elements)
  data_labels = np.asarray(data_labels)
  return data_elements, data_labels

"""## Without augmentation"""

data_elements, data_labels = get_all_mel_spectrograms(Crema_df, augment=False)

datalabels = to_categroical_labels(data_labels)
X_train, X_val, X_test, y_train, y_val, y_test = split_data_into_train_val_test(data_elements, datalabels, 0.3, 0.05, ismel=True)

X_train = X_train[...,np.newaxis]
X_val = X_val[...,np.newaxis]
X_test = X_test[...,np.newaxis]
X_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape

"""## With Augmentation"""

X_train, X_test, y_train, y_test = train_test_split(Crema_df.Path, Crema_df.Emotion, shuffle = True, random_state = 12, test_size = 0.3)
c_df = pd.DataFrame(X_train)
c_df["Emotion"] = y_train

data_elements, data_labels = get_all_mel_spectrograms(c_df, augment=True)

X_train, X_val, y_train, y_val = train_test_split(data_elements,
                                                    data_labels,
                                                    random_state=12,
                                                    test_size=0.05,
                                                    shuffle=False)

from sklearn import utils
X_train, y_train = utils.shuffle(X_train, y_train, random_state=14)
X_val, y_val = utils.shuffle(X_val, y_val, random_state=14)

t_df = pd.DataFrame(X_test)
t_df["Emotion"] = y_test

X_test, y_test = get_all_mel_spectrograms(t_df, augment=False)

y_train = to_categroical_labels(y_train)
y_val = to_categroical_labels(y_val)
y_test = to_categroical_labels(y_test)

X_train = X_train[...,np.newaxis]
X_val = X_val[...,np.newaxis]
X_test = X_test[...,np.newaxis]
X_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape

"""## Augmentation on train only"""

X_train, X_val, X_test, y_train, y_val, y_test = split_data_into_train_val_test(Crema_df.Path, Crema_df.Emotion, 0.3, 0.05, ismel=True)
c_df = pd.DataFrame(X_train)
c_df["Emotion"] = y_train

data_elements, data_labels = get_all_mel_spectrograms(c_df, augment=True)

t_df = pd.DataFrame(X_test)
t_df["Emotion"] = y_test
v_df = pd.DataFrame(X_val)
v_df["Emotion"] = y_val

X_test,y_test = get_all_mel_spectrograms(t_df, augment=False)

X_val,y_val = get_all_mel_spectrograms(v_df, augment=False)

X_train=data_elements
y_train=data_labels
y_test = to_categroical_labels(y_test)
y_train = to_categroical_labels(y_train)
y_val = to_categroical_labels(y_val)
X_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape

X_train = X_train[...,np.newaxis]
X_val = X_val[...,np.newaxis] 
X_test = X_test[...,np.newaxis]
X_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape

"""# Callbacks and visualization functions"""

def scheduler(epoch, lr):
    if epoch <= 25:
      return lr
    else:
      return lr * np.exp(-0.1)

def cosine_schedule(epoch, lr):
  total_epochs = 30
  initial_lr = 0.001
  new_lr = 0.5 * initial_lr * (1 + np.cos(epoch*np.pi / total_epochs))
  return new_lr

import math
def step_decay(epoch):
   initial_lrate = 0.001
   drop = 0.5
   epochs_drop = 7.0
   lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
   return lrate

reduce_on_plateau = ReduceLROnPlateau(monitor='val_loss', 
                                                 factor=0.75, patience=12, 
                                                 verbose=1, mode='max', 
                                                 min_lr=0.00001)
earlystopping = EarlyStopping(monitor ="val_loss", patience = 5, restore_best_weights = True)

LRScheduler = LearningRateScheduler(step_decay, verbose=1)

model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Models/Conv2D_checkpoint', monitor='val_loss', verbose=1, save_best_only=True)

def step_decay2(epoch):
   initial_lrate = 5e-3
   drop = 0.5
   epochs_drop = 7.0
   lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
   return lrate

LRScheduler2 = LearningRateScheduler(step_decay2, verbose=1)

from keras import backend as K
def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision
    
def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

def visualize_loss_and_acc(historys, idx):
  for history in historys:
    fig , ax = plt.subplots(1,2)
    train_acc = history.history['acc']
    train_loss = history.history['loss']
    test_acc = history.history['val_acc']
    test_loss = history.history['val_loss']

    fig.set_size_inches(20,6)
    ax[0].plot(test_loss , label = f'Validation Loss {idx}')
    ax[0].set_title('Validation Loss')
    ax[0].legend()
    ax[0].set_xlabel("Epochs")

    ax[1].plot(train_acc, label = f'Training Accuracy {idx}')
    ax[1].plot(test_acc , label = f'Validation Accuracy {idx}')
    ax[1].set_title('Training & Validation Accuracy')
    ax[1].legend()
    ax[1].set_xlabel("Epochs")
    idx += 1
    plt.show()

"""#Conv1D Model

### Model using ZCR and RMSE, intial learning rate 0f 0.00008
"""

def make_model(X_train):
  model = Sequential()
  model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(X_train.shape[1], 1)))
  model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

  model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))
  model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

  model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))
  model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))
  model.add(Dropout(0.2))

  model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))
  model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

  model.add(Flatten())
  model.add(Dense(units=64, activation='relu'))
  model.add(Dropout(0.3))

  model.add(Dense(units=6, activation='softmax'))
  #this is the best arch try to change learning paramters if you can get better accuracy
  opt = optimizers.Adam(0.00008)
  model.compile(optimizer = opt , loss = 'categorical_crossentropy' ,metrics = ['accuracy', f1_m])
  return model

model = make_model(X_train)
model.summary()

history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=16, callbacks = [])

visualize_loss_and_acc([history], 0)

print("Accuracy of our model on test data : " , model.evaluate(X_test, y_test)[1]*100 , "%")

from tensorflow.keras.models import load_model
# model1 = load_model('drive/MyDrive/Assign_Features/best_model', custom_objects={'f1_m':f1_m})

y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

from sklearn.metrics import classification_report
x = classification_report(y_true, y_pred, target_names=['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad'])
print(x)

import seaborn as sn
from sklearn.metrics import confusion_matrix
classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad']
array = confusion_matrix(y_true, y_pred)
array
df_cm = pd.DataFrame(array, index = classes,
                    columns = classes)
plt.figure(figsize = (12,10))
sn.heatmap(df_cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')
plt.title('Confusion Matrix', size=20)
plt.xlabel('Predicted Labels', size=14)
plt.ylabel('Actual Labels', size=14)
plt.show()

"""## model using ZCR and Energy"""

def make_model(X_train):
  model = Sequential()
  model.add(Conv1D(64, kernel_size=10, strides=1, padding='same', activation='relu', input_shape=(X_train.shape[1], 1)))
  model.add(BatchNormalization())
  model.add(MaxPooling1D(pool_size=5, strides=2))
  model.add(Conv1D(64, kernel_size=5, activation='relu', padding='same'))
  model.add(BatchNormalization())
  model.add(MaxPooling1D(pool_size=5, strides=2))
  model.add(Conv1D(64, kernel_size=3, activation='relu', padding='same'))
  model.add(BatchNormalization())
  model.add(MaxPooling1D(pool_size=2, strides=2))
  model.add(Conv1D(128, kernel_size=3, activation='relu', padding='same'))
  model.add(BatchNormalization())
  model.add(MaxPooling1D(pool_size=3, strides=2))
  model.add(Conv1D(128, kernel_size=3, activation='relu', padding='same'))
  model.add(BatchNormalization())
  model.add(MaxPooling1D(pool_size=2, strides=2))
  model.add(Flatten())
  model.add(Dense(256, activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(128, activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(6, activation='softmax'))
  opt = optimizers.Adam(learning_rate=1e-3)
  model.compile(optimizer=opt, loss="categorical_crossentropy", metrics=["acc", f1_m])
  return model

model = make_model(X_train)
model.summary()

history = model.fit(X_train, y_train, validation_data=(X_val, y_val),
                    epochs=50, batch_size=32, callbacks = [LRScheduler])

visualize_loss_and_acc([history], 0)

print("Accuracy of our model on test data : " , model.evaluate(X_test, y_test)[1]*100 , "%")

from tensorflow.keras.models import load_model
# model1 = load_model('drive/MyDrive/Assign_Features/best_model', custom_objects={'f1_m':f1_m})

y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

from sklearn.metrics import classification_report
x = classification_report(y_true, y_pred)
print(x)

import seaborn as sn
from sklearn.metrics import confusion_matrix
classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad']
array = confusion_matrix(y_true, y_pred)
array
df_cm = pd.DataFrame(array, index = classes,
                    columns = classes)
plt.figure(figsize = (12,10))
sn.heatmap(df_cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')
plt.title('Confusion Matrix', size=20)
plt.xlabel('Predicted Labels', size=14)
plt.ylabel('Actual Labels', size=14)
plt.show()

"""# Conv2D Model (For melspectrogram)"""

def get_2D_model(X_train):
  model = Sequential()
  initializer = initializers.HeUniform()
  weight_decay = regularizers.l2(0)
  model.add(Conv2D(64, kernel_size=(5,5), activation='relu', padding='same', input_shape=(X_train.shape[1:]), kernel_initializer=initializer, kernel_regularizer = weight_decay))
  model.add(BatchNormalization(name="BatchNorm1"))
  model.add(MaxPooling2D(pool_size=(5,5), strides=(2,2), name="MaxPool1"))
  model.add(Conv2D(64, kernel_size=(5,5), activation='relu', padding='same', kernel_initializer=initializer, kernel_regularizer = weight_decay, name="Conv2"))
  model.add(BatchNormalization(name="BatchNorm2"))
  model.add(MaxPooling2D(pool_size=(5,5), strides=(2,2), name="MaxPool2"))
  model.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same', kernel_initializer=initializer, kernel_regularizer = weight_decay, name="Conv3"))
  model.add(BatchNormalization(name="BatchNorm3"))
  model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), name="MaxPool3"))
  model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same', kernel_initializer=initializer, kernel_regularizer = weight_decay, name="Conv4"))
  model.add(BatchNormalization(name="BatchNorm4"))
  model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), name="MaxPool4"))
  model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same', kernel_initializer=initializer, kernel_regularizer = weight_decay, name="Conv5"))
  model.add(BatchNormalization(name="BatchNorm5"))
  model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), name="MaxPool5"))
  model.add(Flatten())
  model.add(Dense(256, activation='relu', kernel_initializer=initializer, kernel_regularizer = weight_decay, name="Dense1"))
  model.add(Dropout(0.5, name="Dropout1"))
  model.add(Dense(128, activation='relu', kernel_initializer=initializer, kernel_regularizer = weight_decay, name="Dense2"))
  model.add(Dropout(0.5, name="Dropout2"))
  model.add(Dense(6, activation='softmax', kernel_initializer=initializer, kernel_regularizer = weight_decay, name="Output_Layer"))
  model.build(input_shape=(X_train.shape))
  opt = optimizers.Adam(0.001)
  model.compile(optimizer=opt, loss="categorical_crossentropy", metrics=["acc", f1_m])
  return model

model3 = get_2D_model(X_train)
model3.summary()

"""### trying out models

### Best Model for Conv2D

#### The above architecture (but 1st Conv2D layer had kernel_size (10,10)) with initial learning rate 0.00025 with step decay by 0.5 every 10 epochs and no regularization
"""

model1 = get_2D_model(X_train)
model1.summary()

history = model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=64, callbacks = [LRScheduler])

visualize_loss_and_acc([history], 0)

print("Accuracy of our model on test data : " , model1.evaluate(X_test, y_test)[1]*100 , "%")

from tensorflow.keras.models import load_model
# model1 = load_model('drive/MyDrive/Assign_Features/best_model', custom_objects={'f1_m':f1_m})

y_pred1 = model1.predict(X_test)
y_predx = np.argmax(y_pred1, axis=1)
y_true = np.argmax(y_test, axis=1)

from sklearn.metrics import classification_report
x = classification_report(y_true, y_predx, target_names=['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad'])
print(x)

import seaborn as sn
from sklearn.metrics import confusion_matrix
classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad']
array = confusion_matrix(y_true, y_predx)
array
df_cm = pd.DataFrame(array, index = classes,
                    columns = classes)
plt.figure(figsize = (12,10))
sn.heatmap(df_cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')
plt.title('Confusion Matrix', size=20)
plt.xlabel('Predicted Labels', size=14)
plt.ylabel('Actual Labels', size=14)
plt.show()

"""#### The above architecture with intial learning rate 0.00085 with reducing on plateau callback and l2 regularization of 0.001"""

model2 = get_2D_model(X_train)
model2.summary()

history = model2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32, callbacks = [reduce_on_plateau])

visualize_loss_and_acc([history], 0)

print("Accuracy of our model on test data : " , model2.evaluate(X_test, y_test)[1]*100 , "%")

y_pred2 = model2.predict(X_test)
y_predx = np.argmax(y_pred2, axis=1)
y_true = np.argmax(y_test, axis=1)

from sklearn.metrics import classification_report
x = classification_report(y_true, y_predx, target_names=['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad'])
print(x)

import seaborn as sn
from sklearn.metrics import confusion_matrix
classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad']
array = confusion_matrix(y_true, y_predx)
array
df_cm = pd.DataFrame(array, index = classes,
                    columns = classes)
plt.figure(figsize = (12,10))
sn.heatmap(df_cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')
plt.title('Confusion Matrix', size=20)
plt.xlabel('Predicted Labels', size=14)
plt.ylabel('Actual Labels', size=14)
plt.show()

"""#### The above architecture (but 1st Conv2D layer had kernel_size (10,10)) with initial learning rate 0.001 with step decay by 0.5 every 7 epochs and no regularization"""

model3 = get_2D_model(X_train)
model3.summary()

history = model3.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=64, callbacks = [LRScheduler])

visualize_loss_and_acc([history], 0)

print("Accuracy of our model on test data : " , model3.evaluate(X_test, y_test)[1]*100 , "%")

y_pred3 = model3.predict(X_test)
y_predx = np.argmax(y_pred3, axis=1)
y_true = np.argmax(y_test, axis=1)

from sklearn.metrics import classification_report
x = classification_report(y_true, y_predx, target_names=['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad'])
print(x)

import seaborn as sn
from sklearn.metrics import confusion_matrix
classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad']
array = confusion_matrix(y_true, y_predx)
array
df_cm = pd.DataFrame(array, index = classes,
                    columns = classes)
plt.figure(figsize = (12,10))
sn.heatmap(df_cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')
plt.title('Confusion Matrix', size=20)
plt.xlabel('Predicted Labels', size=14)
plt.ylabel('Actual Labels', size=14)
plt.show()

"""## Ensemble"""

from scipy.stats import mode
y_pred = []
for i in range(len(y_test)):
  x = [np.argmax(y_pred1, axis=1)[i], np.argmax(y_pred2, axis=1)[i], np.argmax(y_pred3, axis=1)[i]]
  ens = mode(x)[0][0]
  y_pred.append(ens)

y_true = np.argmax(y_test, axis=1)
from sklearn.metrics import classification_report
x = classification_report(y_true, y_pred, target_names=['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad'])
print(x)

import seaborn as sn
from sklearn.metrics import confusion_matrix
classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad']
array = confusion_matrix(y_true, y_pred)
array
df_cm = pd.DataFrame(array, index = classes,
                    columns = classes)
plt.figure(figsize = (12,10))
sn.heatmap(df_cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')
plt.title('Confusion Matrix', size=20)
plt.xlabel('Predicted Labels', size=14)
plt.ylabel('Actual Labels', size=14)
plt.show()
